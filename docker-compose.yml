version: '3.8'

# =============================================================================
# Telephony Service - Complete AI Stack
# =============================================================================
# All services for AI-powered phone conversations:
# - Edge TTS (Text-to-Speech)
# - Whisper STT (Speech-to-Text)
# - Ollama LLM (AI Responses)
# - API Trigger (Call initiation)
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Edge TTS Service (Text-to-Speech)
  # OpenAI-compatible API using Microsoft Edge TTS - FREE
  # ---------------------------------------------------------------------------
  edge-tts:
    image: python:3.11-slim
    container_name: telephony-tts
    restart: unless-stopped
    working_dir: /app
    ports:
      - "127.0.0.1:5050:5050"
    environment:
      - TZ=UTC
    volumes:
      - ./sounds:/app/sounds
      - ./scripts:/app/scripts
    command: >
      bash -c "pip install --no-cache-dir edge-tts flask && python /app/scripts/tts-server.py"
    networks:
      - telephony-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5050/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ---------------------------------------------------------------------------
  # Whisper STT Service (Speech-to-Text)
  # Uses faster-whisper for efficient transcription - FREE
  # ---------------------------------------------------------------------------
  whisper-stt:
    build:
      context: .
      dockerfile: docker/Dockerfile.stt
    container_name: telephony-stt
    restart: unless-stopped
    ports:
      - "127.0.0.1:5051:5051"
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-base}
    volumes:
      - whisper-cache:/root/.cache
    networks:
      - telephony-network
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5051/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Ollama LLM Service (AI Responses)
  # Local LLM using Llama 3.2 - FREE
  # ---------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: telephony-ollama
    restart: unless-stopped
    ports:
      - "127.0.0.1:11434:11434"
    environment:
      - OLLAMA_ORIGINS=*
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - telephony-network
    # Pull the model on first start
    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull ${LLM_MODEL:-llama3.2:1b} && wait"]

  # ---------------------------------------------------------------------------
  # API Trigger Service (Call Initiation)
  # HTTP API to start outbound calls
  # Uses host network to access Asterisk directly
  # ---------------------------------------------------------------------------
  api-trigger:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: telephony-api
    restart: unless-stopped
    network_mode: host
    environment:
      - PORT=3030
      - ASTERISK_HOST=localhost
    volumes:
      - /usr/sbin/asterisk:/usr/sbin/asterisk:ro
      - /var/run/asterisk:/var/run/asterisk
    depends_on:
      - edge-tts
      - whisper-stt
      - ollama

networks:
  telephony-network:
    name: telephony-network
    driver: bridge

volumes:
  whisper-cache:
  ollama-data:
